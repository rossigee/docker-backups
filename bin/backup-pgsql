#!/bin/bash

# TODO: Check inputs

# Incorporate datestamp into filename
filename=$PGSQL_NAME-$(date +%Y%m%d%H%M%S)
s3location=$S3_LOCATION$filename.sql.gpg
#echo "Dumping database '$PGSQL_NAME' to '$s3location'..."

# Dump into an encrypted file
#echo -e "\tDumping..."
dumpstarttime=$(date +%s)
mkfifo /tmp/dumpstream.$$
pg_dump -h $PGSQL_HOST -U $PGSQL_USER $PGSQL_NAME > /tmp/dumpstream.$$ &
if [ $? -ne 0 ]; then
    echo "ERROR: Unable to dump database to FIFO."
    exit 1
fi

#echo -e "\tEncrypting..."
mkfifo /tmp/passphrase.$$
echo $PASSPHRASE > /tmp/passphrase.$$ &
gpg2 --batch --yes -q --passphrase-fd 4 -c - \
        4< /tmp/passphrase.$$ \
        < /tmp/dumpstream.$$ \
        > /tmp/$PGSQL_NAME.sql.gpg
if [ $? -ne 0 ]; then
    echo "ERROR: Unable to encrypt database file."
    exit 1
fi

# Push on up to S3
uploadstarttime=$(date +%s)
aws s3 cp --quiet /tmp/$PGSQL_NAME.sql.gpg $s3location
if [ $? -ne 0 ]; then
    echo "ERROR: Unable to upload database dump to S3."
    exit 1
fi

# Report on size and timings of dump
dumpsize=$(du -sb /tmp/$PGSQL_NAME.sql.gpg | cut -f1)
endtime=$(date +%s)
dumptime=$[uploadstarttime - dumpstarttime]
uploadtime=$[endtime - uploadstarttime]

# Tidy up
rm -f /tmp/dumpstream.$$
rm -f /tmp/passphrase.$$
rm -f /tmp/$PGSQL_NAME.sql.gpg

# Report (Prometheus format)
echo "# HELP backup_size Size of backup file in bytes"
echo "# TYPE backup_size gauge"
echo "backup_size $dumpsize"
echo "# HELP backup_dumptime Time taken to dump and encrypt backup in seconds"
echo "# TYPE backup_dumptime gauge"
echo "backup_dumptime $dumptime"
echo "# HELP backup_uploadtime Time taken to upload backup in seconds"
echo "# TYPE backup_uploadtime gauge"
echo "backup_uploadtime $uploadtime"
echo "# HELP backup_timestamp Time backup completed as seconds-since-the-epoch"
echo "# TYPE backup_timestamp counter"
echo "backup_timestamp $endtime"
